{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pyroomacoustics as pra\n",
    "import soundfile as sf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wandas.core import ChannelFrame\n",
    "from models.overlap_add import OverlapAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイル読み込み (例: LibriSpeech データセット)\n",
    "data_dir = \"./data\"\n",
    "fs = 32000\n",
    "source_audio_files = [\n",
    "    \"target.wav\", \n",
    "    \"noise.wav\", \n",
    "]\n",
    "source_audio_files = [os.path.join(data_dir, f) for f in source_audio_files]\n",
    "assert all(os.path.exists(f) for f in source_audio_files), \"音声ファイルが見つかりません\"\n",
    "source_signals = [sf.read(file)[0][:fs*5] for file in source_audio_files]\n",
    "\n",
    "\n",
    "# 部屋の形状を設定（角の座標）\n",
    "corners = np.array([\n",
    "    [0, 0], [8, 0], [8, 8], [0, 8]\n",
    "]).T  # 転置して [2, N] の形に\n",
    "\n",
    "# 壁の吸収率を設定\n",
    "absorption = 0.4  # 一般壁の吸収率\n",
    "scattering = 0.1  # 散乱率\n",
    "\n",
    "# 部屋の作成\n",
    "room = pra.Room.from_corners(\n",
    "    corners=corners,\n",
    "    fs=fs,\n",
    "    materials=pra.Material(absorption, scattering),\n",
    "    max_order=12  # 反射の最大回数\n",
    ")\n",
    "\n",
    "# マイクを配置\n",
    "mic_positions = np.array([[1.0, 1.0], [7.0, 7.0]]).T  # 2つのマイクの座標\n",
    "room.add_microphone_array(mic_positions)\n",
    "\n",
    "# 音源を配置\n",
    "# 音源配置と信号追加\n",
    "source_positions = [[2.0, 2.0], [6.0, 6.0]]  # 2つの音源の座標\n",
    "for position, signal in zip(source_positions, source_signals):\n",
    "    room.add_source(position, signal=signal)\n",
    "\n",
    "\n",
    "# 描画\n",
    "fig, ax = room.plot()\n",
    "ax.set_title(\"Room with Obstacles and Multiple Microphones/Sources\")\n",
    "plt.show()\n",
    "\n",
    "# シミュレーション実行\n",
    "room.simulate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マイク信号取得\n",
    "mic_signals = room.mic_array.signals\n",
    "\n",
    "# 各音源の残響込み信号を生成（RIRを畳み込み）\n",
    "reverberant_signals = []\n",
    "for i, src in enumerate(room.sources):\n",
    "    rir = room.rir[0][i]  # マイク0と音源iのRIR\n",
    "    reverberant_signal = np.convolve(source_signals[i], rir, mode='same')\n",
    "    reverberant_signals.append(reverberant_signal)\n",
    "reverberant_signals = np.array(reverberant_signals)\n",
    "val_signal = ChannelFrame.from_ndarray(np.vstack((mic_signals[...,:reverberant_signals.shape[-1]], reverberant_signals)), sampling_rate=fs, labels=[\"mixed\", \"noise\"]+[\"tgt\", \"noise_clean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_signal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from wandas.core import ChannelFrame\n",
    "# 'model'ディレクトリをPythonのパスに追加\n",
    "from models.dcunet import TwoChDCUNet\n",
    "\n",
    "from torchmetrics.audio.snr import (\n",
    "    signal_noise_ratio as snr,\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./DCUNet/conf_finetuning.yml\") as f:\n",
    "    conf = yaml.safe_load(f)\n",
    "conf[\"exp_dir\"] = \"../exp/DCUNet_TwoNoise_ClossTalk\"\n",
    "\n",
    "model = TwoChDCUNet(\n",
    "    **conf[\"filterbank\"],\n",
    "    **conf[\"masknet\"],\n",
    "    sample_rate=conf[\"data\"][\"sample_rate\"],\n",
    ")\n",
    "\n",
    "state_dict = torch.load(\"./exp/checkpoints/epoch=66-step=670000.ckpt\", weights_only=True, map_location=\"cpu\")\n",
    "\n",
    "# # システムの状態辞書からモデルの部分だけを抽出\n",
    "model_state_dict = {}\n",
    "for key, value in state_dict[\"state_dict\"].items():\n",
    "    # \"model.\"で始まるキーだけを取り出し、プレフィックスを削除\n",
    "    if key.startswith(\"model.\"):\n",
    "        model_state_dict[key[6:]] = value  # \"model.\" の6文字を削除\n",
    "\n",
    "# モデルに状態辞書を読み込む\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_two_channel_audio(mixed, noise_clean, model, ola):\n",
    "    # 入力形状: [ch, length] = [2, length]\n",
    "    x = np.stack([mixed, noise_clean], axis=0).astype(np.float64)\n",
    "    \n",
    "    def batch_process_with_dnn(segments):\n",
    "        \"\"\"\n",
    "        バッチとしてマルチチャネルセグメントを処理\n",
    "        \n",
    "        Args:\n",
    "            segments: 形状(n_segments, n_channels, window_size)の配列\n",
    "            \n",
    "        Returns:\n",
    "            形状(n_segments, n_channels, window_size)の処理済み配列\n",
    "        \"\"\"\n",
    "        # ダミー処理 - モデルがない場合単に信号を返す\n",
    "        if model is None:\n",
    "            return segments \n",
    "            \n",
    "        # input_signal.shape [1, 2, 320000]\n",
    "        # NumPy -> PyTorch、データ型を合わせる\n",
    "        with torch.no_grad():\n",
    "            device = next(model.parameters()).device\n",
    "            dtype = next(model.parameters()).dtype\n",
    "            \n",
    "            # 入力データをモデルと同じデータ型に変換\n",
    "            tsegs = torch.from_numpy(segments).to(device=device, dtype=dtype)\n",
    "            \n",
    "            # モデルが入力として(batch_size, channels, time)の形状を期待\n",
    "            # 現在の形状は(n_segments, n_channels, window_size)なのでOK\n",
    "            est_targets = model(tsegs).squeeze()\n",
    "            \n",
    "            # 出力変換: ターゲット信号とノイズ信号\n",
    "            est_noise = (tsegs[:,0] - est_targets).squeeze(1)\n",
    "            \n",
    "            # PyTorch -> NumPy\n",
    "            result = torch.stack([est_targets, est_noise], dim=1).cpu().numpy()\n",
    "        return result\n",
    "    \n",
    "    # OverlapAddによる処理\n",
    "    processed = ola.process_signal(x, batch_process_with_dnn)\n",
    "    \n",
    "    # 出力形状: [ch, length] = [2, length]\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed, noise = val_signal[\"mixed\"].data, val_signal[\"noise\"].data\n",
    "tgt = val_signal[\"tgt\"].data\n",
    "mixed = np.tile(mixed, 5)\n",
    "noise_clean = np.tile(noise, 5)\n",
    "tgt = np.tile(tgt, 5)\n",
    "\n",
    "window_size = conf[\"data\"][\"sample_rate\"]*3\n",
    "hop_size = window_size//2\n",
    "ola = OverlapAdd(window_size=window_size, hop_size=hop_size, window='hann')\n",
    "\n",
    "processed=process_two_channel_audio(mixed, noise_clean, model, ola)\n",
    "sep_signal_segment = ChannelFrame.from_ndarray(np.stack([mixed, noise_clean, tgt, processed[0], processed[1]], axis=0), sampling_rate=conf[\"data\"][\"sample_rate\"], labels=[\"mixed\", \"noise_clean\", \"tgt\", \"est_tgt\", \"est_noise\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in sep_signal_segment:\n",
    "    print(f\"{ch.label} snr:{snr(torch.from_numpy(ch.data), torch.from_numpy(tgt))}\")\n",
    "sep_signal_segment.to_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in sep_signal_segment:\n",
    "    ch.stft().plot(vmin=0, vmax=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
